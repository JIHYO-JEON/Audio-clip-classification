{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data brought train\n",
      "(48840, 60, 41, 2) (48840, 10)\n",
      "data brought test\n",
      "(5218, 60, 41, 2) (5218, 10)\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "\n",
    "import h5py\n",
    "\n",
    "def bringdata(filename):\n",
    "    hdf5Path = filename\n",
    "    dataset = h5py.File(hdf5Path, 'r')\n",
    "    data = dataset['audio']\n",
    "    labels = dataset['labels']\n",
    "    print 'data brought', filename\n",
    "    print data.shape, labels.shape\n",
    "    return data, labels\n",
    "\n",
    "traindata, trainlabel = bringdata('train')\n",
    "testdata, testlabel = bringdata('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "plt.rcParams['font.family'] = 'serif'\n",
    "plt.rcParams['font.serif'] = 'Ubuntu'\n",
    "plt.rcParams['font.monospace'] = 'Ubuntu Mono'\n",
    "plt.rcParams['font.size'] = 12\n",
    "plt.rcParams['axes.labelsize'] = 11\n",
    "plt.rcParams['axes.labelweight'] = 'bold'\n",
    "plt.rcParams['axes.titlesize'] = 12\n",
    "plt.rcParams['xtick.labelsize'] = 9\n",
    "plt.rcParams['ytick.labelsize'] = 9\n",
    "plt.rcParams['legend.fontsize'] = 11\n",
    "plt.rcParams['figure.titlesize'] = 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "frames = 41\n",
    "bands = 60\n",
    "num_channels = 2\n",
    "\n",
    "#feature_size = 2460 #60x41\n",
    "num_labels = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None,bands,frames,num_channels])\n",
    "Y = tf.placeholder(tf.float32, shape=[None,num_labels])\n",
    "\n",
    "keep_prob1 = tf.placeholder(tf.float32)  # use for dropout\n",
    "keep_prob2 = tf.placeholder(tf.float32)\n",
    "\n",
    "# conv1\n",
    "with tf.variable_scope('conv1') as scope:\n",
    "    weights = tf.get_variable('weights', shape=[5, 5, 2, 256], dtype=tf.float32,\n",
    "                              initializer=tf.contrib.layers.xavier_initializer(seed=0))\n",
    "\n",
    "    biases = tf.get_variable('biases', shape=[256], dtype=tf.float32,\n",
    "                             initializer=tf.constant_initializer(0.1))\n",
    "\n",
    "    conv1 = tf.nn.relu(tf.nn.conv2d(X, weights, strides=[1, 1, 1, 1], padding='SAME') + biases)\n",
    "\n",
    "# pool1\n",
    "with tf.variable_scope('pool1') as scope:\n",
    "    pool1 = tf.nn.max_pool(conv1, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1],\n",
    "                           padding='VALID', name='pool1')\n",
    "# conv2\n",
    "with tf.variable_scope('conv2') as scope:\n",
    "    weights = tf.get_variable('weights', shape=[3, 3, 256, 384], dtype=tf.float32,\n",
    "                              initializer=tf.contrib.layers.xavier_initializer(seed=0))\n",
    "\n",
    "    biases = tf.get_variable('biases', shape=[384], dtype=tf.float32,\n",
    "                             initializer=tf.constant_initializer(0.1))\n",
    "\n",
    "    conv2 = tf.nn.relu(tf.nn.conv2d(pool1, weights, strides=[1, 1, 1, 1], padding='SAME') + biases)\n",
    "    conv2 = tf.nn.local_response_normalization(conv2, depth_radius=5.0, bias=2.0, alpha=1e-4, beta=0.75)\n",
    "\n",
    "# conv3\n",
    "with tf.variable_scope('conv3') as scope:\n",
    "    weights = tf.get_variable('weights', shape=[3, 3, 384, 384], dtype=tf.float32,\n",
    "                              initializer=tf.contrib.layers.xavier_initializer(seed=0))\n",
    "\n",
    "    biases = tf.get_variable('biases', shape=[384], dtype=tf.float32,\n",
    "                             initializer=tf.constant_initializer(0.1))\n",
    "\n",
    "    conv3 = tf.nn.relu(tf.nn.conv2d(conv2, weights, strides=[1, 1, 1, 1], padding='SAME') + biases)\n",
    "\n",
    "# conv4\n",
    "with tf.variable_scope('conv4') as scope:\n",
    "    weights = tf.get_variable('weights', shape=[3, 3, 384, 256], dtype=tf.float32,\n",
    "                              initializer=tf.contrib.layers.xavier_initializer(seed=0))\n",
    "\n",
    "    biases = tf.get_variable('biases', shape=[256], dtype=tf.float32,\n",
    "                             initializer=tf.constant_initializer(0.1))\n",
    "\n",
    "    conv4 = tf.nn.relu(tf.nn.conv2d(conv3, weights, strides=[1, 1, 1, 1], padding='SAME') + biases)\n",
    "\n",
    "# pool2\n",
    "with tf.variable_scope('pool2') as scope:\n",
    "    pool2 = tf.nn.max_pool(conv4, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1],\n",
    "                           padding='VALID', name='pool5')\n",
    "pool2_flatten = tf.contrib.layers.flatten(pool2)  # flatten pool3 to shape of (128,16*16*32)\n",
    "\n",
    "# fully connected layer 1 and dropout\n",
    "fc1 = tf.contrib.layers.fully_connected(pool2_flatten, 256, activation_fn=tf.nn.relu)\n",
    "\n",
    "# fc1_drop = tf.nn.dropout(fc1, keep_prob1)\n",
    "\n",
    "# fully connected layer 2 and dropout\n",
    "fc2 = tf.contrib.layers.fully_connected(fc1, 256, activation_fn=tf.nn.relu)\n",
    "\n",
    "fc2_drop = tf.nn.dropout(fc2, keep_prob2)\n",
    "\n",
    "# fully connected layer 3\n",
    "fc3 = tf.contrib.layers.fully_connected(fc2_drop, num_labels, activation_fn=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = fc3, labels = Y))\n",
    "train_optimizer = tf.train.GradientDescentOptimizer(0.001).minimize(cost)\n",
    "cost_history = np.empty(shape=[1], dtype=float)\n",
    "\n",
    "# accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(fc3,1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, 'float'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-11-68f56f388e49>:7: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "0 / 100 done cost: 1.597317110488675\n",
      "1 / 100 done cost: 1.2347905382687705\n",
      "2 / 100 done cost: 1.0092768859451022\n",
      "3 / 100 done cost: 0.8564360168685733\n",
      "4 / 100 done cost: 0.7420460582557998\n",
      "5 / 100 done cost: 0.6505663876678278\n",
      "6 / 100 done cost: 0.5757687758198511\n",
      "7 / 100 done cost: 0.5272158701579275\n",
      "8 / 100 done cost: 0.4933683366152996\n",
      "9 / 100 done cost: 0.4245617244835294\n",
      "10 / 100 done cost: 0.3646092247549454\n",
      "11 / 100 done cost: 0.33455748209122516\n",
      "12 / 100 done cost: 0.3227030589917112\n",
      "13 / 100 done cost: 0.2810479159270921\n",
      "14 / 100 done cost: 0.23881519315703897\n",
      "15 / 100 done cost: 0.20978090447116712\n",
      "16 / 100 done cost: 0.20568745688058337\n",
      "17 / 100 done cost: 0.17795000664858876\n",
      "18 / 100 done cost: 0.19440308775394308\n",
      "19 / 100 done cost: 0.1555656609347924\n",
      "20 / 100 done cost: 0.13765910433001066\n",
      "21 / 100 done cost: 0.11382994652631363\n",
      "22 / 100 done cost: 0.1247320983431453\n",
      "23 / 100 done cost: 0.09993798537150866\n",
      "24 / 100 done cost: 0.11011270524289299\n",
      "25 / 100 done cost: 0.09987239103653674\n",
      "26 / 100 done cost: 0.09873129045308299\n",
      "27 / 100 done cost: 0.08661377354791908\n",
      "28 / 100 done cost: 0.08373690267345679\n",
      "29 / 100 done cost: 0.06354139484102043\n",
      "30 / 100 done cost: 0.0636897323870436\n",
      "31 / 100 done cost: 0.058469550069102344\n",
      "32 / 100 done cost: 0.0692599591154809\n",
      "33 / 100 done cost: 0.05237396867929267\n",
      "34 / 100 done cost: 0.044872202273731926\n",
      "35 / 100 done cost: 0.06941945449627054\n",
      "36 / 100 done cost: 0.04406880477527969\n",
      "37 / 100 done cost: 0.03768399979629374\n",
      "38 / 100 done cost: 0.040827350015662366\n",
      "39 / 100 done cost: 0.045335466678816286\n",
      "40 / 100 done cost: 0.05468274922738794\n",
      "41 / 100 done cost: 0.04894236833893675\n",
      "42 / 100 done cost: 0.04123538650482869\n",
      "43 / 100 done cost: 0.031794460372333334\n",
      "44 / 100 done cost: 0.041716630673589945\n",
      "45 / 100 done cost: 0.02159108866923878\n",
      "46 / 100 done cost: 0.044492768605637494\n",
      "47 / 100 done cost: 0.033286051545615064\n",
      "48 / 100 done cost: 0.024681028266482365\n",
      "49 / 100 done cost: 0.019952880208574806\n",
      "50 / 100 done cost: 0.017910181454090332\n",
      "51 / 100 done cost: 0.015312749529943888\n",
      "52 / 100 done cost: 0.026318420891017633\n",
      "53 / 100 done cost: 0.018146616910619563\n",
      "54 / 100 done cost: 0.03536267809427491\n",
      "55 / 100 done cost: 0.023351697975275865\n",
      "56 / 100 done cost: 0.02208994695901683\n",
      "57 / 100 done cost: 0.02092829807288669\n",
      "58 / 100 done cost: 0.0168827354374383\n",
      "59 / 100 done cost: 0.020243578802432508\n",
      "60 / 100 done cost: 0.01655003975550459\n",
      "61 / 100 done cost: 0.01236539052836068\n",
      "62 / 100 done cost: 0.02936763632411986\n",
      "63 / 100 done cost: 0.01988737020094822\n",
      "64 / 100 done cost: 0.020254158936795306\n",
      "65 / 100 done cost: 0.029280846849408206\n",
      "66 / 100 done cost: 0.009084907190384319\n",
      "67 / 100 done cost: 0.017547203166090805\n",
      "68 / 100 done cost: 0.017171256195104973\n",
      "69 / 100 done cost: 0.014833540864128405\n",
      "70 / 100 done cost: 0.0167574879536893\n",
      "71 / 100 done cost: 0.012990195162342809\n",
      "72 / 100 done cost: 0.006687222038491495\n",
      "73 / 100 done cost: 0.006681972864122886\n",
      "74 / 100 done cost: 0.03240542820309592\n",
      "75 / 100 done cost: 0.010113153796842772\n",
      "76 / 100 done cost: 0.01382683442942719\n",
      "77 / 100 done cost: 0.014908616537545674\n",
      "78 / 100 done cost: 0.01353898760209771\n",
      "79 / 100 done cost: 0.006293308427906949\n",
      "80 / 100 done cost: 0.010382933655960842\n",
      "81 / 100 done cost: 0.007789076809733007\n",
      "82 / 100 done cost: 0.007313851528010839\n",
      "83 / 100 done cost: 0.012347006188156\n",
      "84 / 100 done cost: 0.012762826123898783\n",
      "85 / 100 done cost: 0.012695539270808433\n",
      "86 / 100 done cost: 0.04247790811700048\n",
      "87 / 100 done cost: 0.022937438022895464\n",
      "88 / 100 done cost: 0.007795145738572362\n",
      "89 / 100 done cost: 0.005605698620222319\n",
      "90 / 100 done cost: 0.010423077768277422\n",
      "91 / 100 done cost: 0.01751870578620519\n",
      "92 / 100 done cost: 0.00939367612443377\n",
      "93 / 100 done cost: 0.006874599056697618\n",
      "94 / 100 done cost: 0.008239800973775618\n",
      "95 / 100 done cost: 0.019059746798009453\n",
      "96 / 100 done cost: 0.005988819530906745\n",
      "97 / 100 done cost: 0.0040327713224631205\n",
      "98 / 100 done cost: 0.003618982144527834\n",
      "99 / 100 done cost: 0.003506524505769678\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "'numpy.float32' object does not support item assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-68f56f388e49>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mtestbatch_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtestdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moffset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moffset\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mtestbatch_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtestlabel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moffset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moffset\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0macc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtestbatch_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtestbatch_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_prob1\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_prob2\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0macc_total\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'numpy.float32' object does not support item assignment"
     ]
    }
   ],
   "source": [
    "batch_size = 8\n",
    "iterations = 100\n",
    "\n",
    "recall_path = \"model/\"\n",
    "\n",
    "with tf.Session() as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "\n",
    "    # Restore the network\n",
    "    if tf.train.get_checkpoint_state(recall_path): \n",
    "        saver = tf.train.Saver()\n",
    "        saver.restore(session, recall_path)\n",
    "        print(\"good!\")\n",
    "    \n",
    "    # Training\n",
    "    for itr in range(iterations):\n",
    "\n",
    "        c_sum = 0\n",
    "        \n",
    "        for i in range(48840/batch_size):\n",
    "            offset = (i * batch_size) % (trainlabel.shape[0] - batch_size)\n",
    "            batch_x = traindata[offset:(offset + batch_size), :, :, :]\n",
    "            batch_y = trainlabel[offset:(offset + batch_size), :]\n",
    "\n",
    "            _, c = session.run([train_optimizer, cost], feed_dict={X: batch_x, Y: batch_y, keep_prob1: 0.5, keep_prob2: 1.0})\n",
    "            c_sum += c / (48840/batch_size)\n",
    "\n",
    "        cost_history = np.append(cost_history, c_sum)\n",
    "\n",
    "        print itr, '/', iterations, 'done', 'cost:', c_sum\n",
    "\n",
    "        if itr % 5 == 0:\n",
    "            saver = tf.train.Saver()\n",
    "            saver.save(session, recall_path, write_meta_graph=False)\n",
    "        \n",
    "    fig = plt.figure(figsize=(15, 10))\n",
    "    plt.plot(cost_history=True)\n",
    "    plt.axis([0, iterations, 0, np.max(cost_history)])\n",
    "    plt.show()\n",
    "\n",
    "    # Test\n",
    "    acc_total = []\n",
    "    for i in range(3):\n",
    "        offset = (i * batch_size) % (trainlabel.shape[0] - batch_size)\n",
    "        testbatch_x = testdata[offset:(offset + batch_size), :, :, :]\n",
    "        testbatch_y = testlabel[offset:(offset + batch_size), :]\n",
    "        acc[i] = session.run(accuracy, feed_dict={X: testbatch_x, Y: testbatch_y, keep_prob1: 1.0, keep_prob2: 1.0})\n",
    "\n",
    "    print acc_total\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
